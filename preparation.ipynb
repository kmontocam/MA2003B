{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor\n",
    "import unidecode\n",
    "import pickle\n",
    "import os\n",
    "import operator\n",
    "import functools\n",
    "import itertools\n",
    "from typing import Callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=438184844244-uhk0l93iq5rfack31hmfbp1ted62o9o3.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n",
      "Data was correctly allocated in memory\n"
     ]
    }
   ],
   "source": [
    "if 'databases.pkl' not in os.listdir('./raw-data/'):\n",
    "    from quickstart.loader import XlsxDriveLoader\n",
    "\n",
    "    Loads = XlsxDriveLoader()  # Drive Folder is hardcoded in module, since this is not prone to change\n",
    "    databases = Loads.content  # Process takes approximately ~3 minutes to run for the first time.\n",
    "                               # Then the file will be stored in the data/\n",
    "else:\n",
    "    with open('./raw-data/databases.pkl', 'rb') as file:\n",
    "        databases = pickle.load(file)\n",
    "    print(\"Loaded data from .pkl file\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anotaciones sobre banderas de contaminantes.xlsx',\n",
       " 'BD_Tec_Banderas_2018_2021_3Estaciones.xlsx',\n",
       " 'BD_Tec_Banderas_meteo.xlsx',\n",
       " 'BD_Tec_banderas_contaminantes.xlsx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(databases.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(db_name_anotaciones, db_name_meteoorologicas,\n",
    " db_name_meteoorologicas, db_name_contaminantes) = databases.keys()\n",
    "\n",
    "\n",
    "get_col_names: Callable[[pd.core.frame.DataFrame], pd.core.indexes.base.Index] = (\n",
    "    lambda df: sorted(list(df.columns)))\n",
    "\n",
    "\n",
    "def shared_structure(db_name: str) -> list:\n",
    "    \"\"\"Returns sheets in dict of DataFrames that conain the same fields\"\"\"\n",
    "\n",
    "    df_col_names = [[key, get_col_names(val)]\n",
    "                        for key, val in databases[db_name].items()]\n",
    "\n",
    "    grouped = list()\n",
    "    for _, g in itertools.groupby(df_col_names, operator.itemgetter(1)):\n",
    "        group = list(g)\n",
    "        if len(group) > 1:  # Can't be performed with comprehension since g is a generator/iterator\n",
    "            grouped.append(group)\n",
    "\n",
    "    if len(grouped) == 1:\n",
    "        return [sheet_name for sheet_name, _ in functools.reduce(operator.iconcat, grouped, [])]\n",
    "        # Expecting there is only one shared_structure across the DataFrames\n",
    "    else:\n",
    "        return ...  # Not necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_contaminantes = pd.concat([databases[db_name_contaminantes][sheet]\n",
    "                               for sheet in shared_structure(db_name_contaminantes)])\n",
    "\n",
    "raw_meteorologicas = pd.concat([databases[db_name_meteoorologicas][sheet]\n",
    "                                for sheet in shared_structure(db_name_meteoorologicas)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for categorical values\n",
    "\n",
    "identifier = (\n",
    "    databases[db_name_anotaciones]['LEEME']\n",
    "    .loc[:, ['Flag', 'Hora']]\n",
    "    .set_index('Flag')\n",
    "    .dropna(axis='index')\n",
    "    .squeeze()\n",
    "    .apply(lambda string: unidecode.unidecode(string).strip().lower())\n",
    "    .apply(lambda validity: True if validity == 'valida' else False)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "identifier |= {'x': False}  # Record does not appear on DataFrame\n",
    "\n",
    "\n",
    "def f_identifier(dict_): return identifier.get(dict_, True)\n",
    "\n",
    "# Dictionary for mesaurement units\n",
    "\n",
    "measurement_units = (\n",
    "    databases[db_name_anotaciones]['Hoja1']\n",
    "    .iloc[23:41, [0, 2]]\n",
    "    .dropna()\n",
    "    .drop(33)\n",
    "    .pivot(columns='Notas a considerar:', values='Unnamed: 2')\n",
    "    .pipe(janitor.clean_names, remove_special=True)\n",
    "    .mode()\n",
    "    .squeeze()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "with open('data/measurement-units.pkl', 'wb') as file:\n",
    "    pickle.dump(measurement_units, file) # requiered for plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'SO2' # San Pedro identifier\n",
    "ids = [f'{id}', f'{id} b'] \n",
    "\n",
    "fields = ['parametro', 'Fecha']\n",
    "fields.extend(ids)\n",
    "\n",
    "\n",
    "def munge(df: pd.core.frame.DataFrame, fields: list):\n",
    "    return (\n",
    "        df\n",
    "        .loc[:, fields]\n",
    "        .pipe(janitor.rename_columns, new_column_names={'parametro': 'factor',\n",
    "                                                        'Fecha': 'date'})\n",
    "        .pipe(janitor.process_text, column_name='factor', string_function='strip')\n",
    "        .pivot(index='date', columns='factor', values=ids)\n",
    "        .convert_dtypes(convert_integer=False)\n",
    "        .apply(lambda field: field.where(field > 0, pd.NA) if pd.api.types.is_numeric_dtype(field)\n",
    "               else field.map(f_identifier))  # Remove negative values from numeric fields\n",
    "                                              # and map str typed variables with a dict\n",
    "        .pipe(janitor.clean_names, strip_underscores='r')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contaminantes = munge(raw_contaminantes, fields)\n",
    "meteorologicas = munge(raw_meteorologicas, fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Timestamp('2019-05-15 20:00:00')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(contaminantes.index).difference(set(meteorologicas.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a Timestamp missing in `metereologicas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = contaminantes.join(other=meteorologicas,\n",
    "                         how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, flags = df_.columns.droplevel(1).unique()\n",
    "\n",
    "df = (\n",
    "    df_\n",
    "    .apply(lambda row: row[vals].where(row[flags].astype('bool')), axis=1) # check validity\n",
    "    .replace({pd.NA: np.nan}) # .astype() does not operate when having diferent dtypes for\n",
    "                              # missing values\n",
    "    .astype(float)\n",
    "    .drop_duplicates()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping outliers is not as straightforward as it may be in other types of analysis, since we are trying to understand when this spikes on pollution occur. Likewise, imputation for missing values is more than replacing `pd.NA`'s with the mean/median for every column, because we are dealing with a time-series. Data visualization will proceed, to understand the behaviour of data and apply the most adequate procedures for outliers and missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw-data/san-pedro-201701-202106.pkl', 'wb') as file:\n",
    "    pickle.dump(df, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat procedure for region: Centro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'CE' # Centro identifier\n",
    "ids = [f'{id}', f'{id} b'] \n",
    "\n",
    "fields = ['parametro', 'Fecha']\n",
    "fields.extend(ids)\n",
    "\n",
    "contaminantes = munge(raw_contaminantes, fields)\n",
    "meteorologicas = munge(raw_meteorologicas, fields)\n",
    "\n",
    "df_ = contaminantes.join(other=meteorologicas,\n",
    "                         how='outer')\n",
    "\n",
    "vals, flags = df_.columns.droplevel(1).unique()\n",
    "\n",
    "df = (\n",
    "    df_\n",
    "    .apply(lambda row: row[vals].where(row[flags].astype('bool')), axis=1) # check validity\n",
    "    .replace({pd.NA: np.nan}) # .astype() does not operate when having diferent dtypes for\n",
    "                              # missing values\n",
    "    .astype(float)\n",
    "    .drop_duplicates()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw-data/centro-201701-202106.pkl', 'wb') as file:\n",
    "    pickle.dump(df, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664d3ff23a96e90f385c1b0a444556e65eef679ecf6e77192d7d9f6bb9ab3935"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
